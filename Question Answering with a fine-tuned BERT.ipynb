{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8389c65-5c73-46f0-981f-102d25fa217e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from transformers) (2.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from requests->transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc80a43b-28f6-4d2d-be90-0bacbd2f9eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: torch in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zeyne\\anaconda3\\envs\\nlp-\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f69ceb52-9372-4aa0-a048-6bf2a7188817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zeyne\\anaconda3\\envs\\nlp-\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "028cc60a-0485-4e5f-a93e-fa589f948783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'wikipedia', 'id': '3zotghdk5ibi9ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'cnn', 'id': '3wj1oxy92agboo5nlq4r7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'gutenberg', 'id': '3bdcf01ogxu7zdn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'cnn', 'id': '3ewijtffvo7wwchw6rtya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'gutenberg', 'id': '3urfvvm165iantk...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version                                               data\n",
       "0        1  {'source': 'wikipedia', 'id': '3zotghdk5ibi9ce...\n",
       "1        1  {'source': 'cnn', 'id': '3wj1oxy92agboo5nlq4r7...\n",
       "2        1  {'source': 'gutenberg', 'id': '3bdcf01ogxu7zdn...\n",
       "3        1  {'source': 'cnn', 'id': '3ewijtffvo7wwchw6rtya...\n",
       "4        1  {'source': 'gutenberg', 'id': '3urfvvm165iantk..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coqa = pd.read_json('http://downloads.cs.stanford.edu/nlp/data/coqa/coqa-train-v1.0.json')\n",
    "coqa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b07e83-1848-412a-b9ef-8bb52ef87c4d",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd73dd18-010d-4d3d-acc8-8aafd1c7baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del coqa[\"version\"]   #yalnızca data column i ile çalışacağız version column unu siliyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df8ccb47-4a5d-46b5-874f-745ee0e31690",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=[\"text\",\"question\",\"answer\"]\n",
    "\n",
    "comp_list=[]\n",
    "\n",
    "for index, row in coqa.iterrows():         #pandas ın bir fonksiyonu dataframe in her satırını tek tek döner\n",
    "    for i in range(len(row[\"data\"][\"questions\"])):\n",
    "        temp_list=[]\n",
    "        temp_list.append(row[\"data\"][\"story\"])\n",
    "        temp_list.append(row[\"data\"][\"questions\"][i][\"input_text\"])\n",
    "        temp_list.append(row[\"data\"][\"answers\"][i][\"input_text\"])\n",
    "        comp_list.append(temp_list)\n",
    "\n",
    "new_df=pd.DataFrame(comp_list,columns=cols)\n",
    "new_df.to_csv(\"CoQA_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "735dd283-66d9-4bb8-9a23-630e7e2a5fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>When was the Vat formally opened?</td>\n",
       "      <td>It was formally established in 1475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>what is the library for?</td>\n",
       "      <td>research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>for what subjects?</td>\n",
       "      <td>history, and law</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>and?</td>\n",
       "      <td>philosophy, science and theology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>what was started in 2014?</td>\n",
       "      <td>a  project</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The Vatican Apostolic Library (), more commonl...   \n",
       "1  The Vatican Apostolic Library (), more commonl...   \n",
       "2  The Vatican Apostolic Library (), more commonl...   \n",
       "3  The Vatican Apostolic Library (), more commonl...   \n",
       "4  The Vatican Apostolic Library (), more commonl...   \n",
       "\n",
       "                            question                               answer  \n",
       "0  When was the Vat formally opened?  It was formally established in 1475  \n",
       "1           what is the library for?                             research  \n",
       "2                 for what subjects?                     history, and law  \n",
       "3                               and?     philosophy, science and theology  \n",
       "4          what was started in 2014?                           a  project  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"CoQA_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14b93d6f-1bd6-48ab-8f48-9be797c3babc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions and answers 108647\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of questions and answers\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d973ca2-71ec-4412-aac3-e86547575b0a",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ac1ec8d-246a-4dd6-85dd-d271541dc478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model=BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "tokenizer=BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef8a7ff-56d8-4487-8410-9478e7eb0bc8",
   "metadata": {},
   "source": [
    "Asking Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d06ed75f-2446-4957-96ff-06d84869efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_num=np.random.randint(0,len(data))\n",
    "\n",
    "question=data[\"question\"][random_num]\n",
    "text=data[\"text\"][random_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d81e425-7ce5-4c4c-b44f-6c9e2423f6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input has a total of 395 tokens\n"
     ]
    }
   ],
   "source": [
    "input_ids=tokenizer.encode(question,text)\n",
    "print(\"The input has a total of {} tokens\".format(len(input_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0102fb7d-334d-4026-b7f8-a12adcc1d43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]        101\n",
      "many       2,116\n",
      "guests     6,368\n",
      "are        2,024\n",
      "from       2,013\n",
      "what       2,054\n",
      "?          1,029\n",
      "[SEP]        102\n",
      "soccer     4,715\n",
      "star       2,732\n",
      "david      2,585\n",
      "beck      10,272\n",
      "##ham      3,511\n",
      "will       2,097\n",
      "be         2,022\n",
      "there      2,045\n",
      "with       2,007\n",
      "his        2,010\n",
      "pop        3,769\n",
      "star       2,732\n",
      "wife       2,564\n",
      "victoria   3,848\n",
      ".          1,012\n",
      "elton     19,127\n",
      "john       2,198\n",
      "is         2,003\n",
      "attending   7,052\n",
      "with       2,007\n",
      "partner    4,256\n",
      "david      2,585\n",
      "fur        6,519\n",
      "##nish    24,014\n",
      ".          1,012\n",
      "the        1,996\n",
      "guest      4,113\n",
      "list       2,862\n",
      "for        2,005\n",
      "the        1,996\n",
      "april      2,258\n",
      "29         2,756\n",
      "union      2,586\n",
      "of         1,997\n",
      "prince     3,159\n",
      "william    2,520\n",
      "and        1,998\n",
      "kate       5,736\n",
      "middleton  17,756\n",
      "is         2,003\n",
      "still      2,145\n",
      "being      2,108\n",
      "kept       2,921\n",
      "secret     3,595\n",
      ",          1,010\n",
      "but        2,021\n",
      "details    4,751\n",
      "have       2,031\n",
      "begun      5,625\n",
      "to         2,000\n",
      "leak      17,271\n",
      "out        2,041\n",
      ",          1,010\n",
      "with       2,007\n",
      "some       2,070\n",
      "coming     2,746\n",
      "forward    2,830\n",
      "to         2,000\n",
      "say        2,360\n",
      "they       2,027\n",
      "are        2,024\n",
      "attending   7,052\n",
      "and        1,998\n",
      "the        1,996\n",
      "mail       5,653\n",
      "on         2,006\n",
      "sunday     4,465\n",
      "newspaper   3,780\n",
      "claiming   6,815\n",
      "to         2,000\n",
      "have       2,031\n",
      "the        1,996\n",
      "official   2,880\n",
      "invitation   8,468\n",
      "roster     9,238\n",
      ".          1,012\n",
      "the        1,996\n",
      "palace     4,186\n",
      "dismissed   7,219\n",
      "the        1,996\n",
      "newspaper   3,780\n",
      "'          1,005\n",
      "s          1,055\n",
      "list       2,862\n",
      "as         2,004\n",
      "speculation  12,143\n",
      "sunday     4,465\n",
      ".          1,012\n",
      "it         2,009\n",
      "won        2,180\n",
      "'          1,005\n",
      "t          1,056\n",
      "be         2,022\n",
      "clear      3,154\n",
      "until      2,127\n",
      "the        1,996\n",
      "day        2,154\n",
      "how        2,129\n",
      "the        1,996\n",
      "royal      2,548\n",
      "couple     3,232\n",
      "has        2,038\n",
      "balanced  12,042\n",
      "the        1,996\n",
      "protocol   8,778\n",
      "demands    7,670\n",
      "that       2,008\n",
      "they       2,027\n",
      "invite    13,260\n",
      "states     2,163\n",
      "##men      3,549\n",
      ",          1,010\n",
      "diplomats  23,473\n",
      ",          1,010\n",
      "religious   3,412\n",
      "leaders    4,177\n",
      ",          1,010\n",
      "politicians   8,801\n",
      "and        1,998\n",
      "the        1,996\n",
      "like       2,066\n",
      "with       2,007\n",
      "invitations  29,492\n",
      "to         2,000\n",
      "the        1,996\n",
      "people     2,111\n",
      "they       2,027\n",
      "really     2,428\n",
      "want       2,215\n",
      "to         2,000\n",
      "see        2,156\n",
      ",          1,010\n",
      "particularly   3,391\n",
      "the        1,996\n",
      "crowd      4,306\n",
      "they       2,027\n",
      "made       2,081\n",
      "friends    2,814\n",
      "with       2,007\n",
      "when       2,043\n",
      "they       2,027\n",
      "met        2,777\n",
      "and        1,998\n",
      "fell       3,062\n",
      "in         1,999\n",
      "love       2,293\n",
      "at         2,012\n",
      "st         2,358\n",
      ".          1,012\n",
      "andrews    9,261\n",
      "university   2,118\n",
      "in         1,999\n",
      "scotland   3,885\n",
      ".          1,012\n",
      "kate       5,736\n",
      "rear       4,373\n",
      "##don      5,280\n",
      ",          1,010\n",
      "editor     3,559\n",
      "of         1,997\n",
      "high       2,152\n",
      "-          1,011\n",
      "society    2,554\n",
      "magazine   2,932\n",
      "ta        11,937\n",
      "##tler    25,091\n",
      ",          1,010\n",
      "said       2,056\n",
      "many       2,116\n",
      "_          1,035\n",
      "brit      28,101\n",
      "##ons      5,644\n",
      "acted      6,051\n",
      "as         2,004\n",
      "if         2,065\n",
      "they       2,027\n",
      "didn       2,134\n",
      "'          1,005\n",
      "t          1,056\n",
      "really     2,428\n",
      "care       2,729\n",
      "about      2,055\n",
      "receiving   4,909\n",
      "an         2,019\n",
      "invitation   8,468\n",
      "while      2,096\n",
      "secretly  10,082\n",
      "checking   9,361\n",
      "the        1,996\n",
      "mail       5,653\n",
      "every      2,296\n",
      "day        2,154\n",
      "to         2,000\n",
      "see        2,156\n",
      "if         2,065\n",
      "the        1,996\n",
      "invitation   8,468\n",
      "had        2,018\n",
      "arrived    3,369\n",
      ".          1,012\n",
      "\"          1,000\n",
      "everyone   3,071\n",
      "'          1,005\n",
      "s          1,055\n",
      "been       2,042\n",
      "hoping     5,327\n",
      ",          1,010\n",
      "\"          1,000\n",
      "she        2,016\n",
      "said       2,056\n",
      ".          1,012\n",
      "william    2,520\n",
      "and        1,998\n",
      "middleton  17,756\n",
      "have       2,031\n",
      "showed     3,662\n",
      "their      2,037\n",
      "modern     2,715\n",
      "side       2,217\n",
      "by         2,011\n",
      "inviting  15,085\n",
      "a          1,037\n",
      "number     2,193\n",
      "of         1,997\n",
      "close      2,485\n",
      "friends    2,814\n",
      ",          1,010\n",
      "including   2,164\n",
      "some       2,070\n",
      "former     2,280\n",
      "sweetheart  12,074\n",
      "##s        2,015\n",
      ",          1,010\n",
      "the        1,996\n",
      "newspaper   3,780\n",
      "said       2,056\n",
      ".          1,012\n",
      "the        1,996\n",
      "wedding    5,030\n",
      "is         2,003\n",
      "not        2,025\n",
      "technically  10,892\n",
      "a          1,037\n",
      "state      2,110\n",
      "event      2,724\n",
      ",          1,010\n",
      "which      2,029\n",
      "somewhat   5,399\n",
      "limits     6,537\n",
      "the        1,996\n",
      "protocol   8,778\n",
      "requirements   5,918\n",
      "applied    4,162\n",
      "to         2,000\n",
      "the        1,996\n",
      "guest      4,113\n",
      "list       2,862\n",
      ".          1,012\n",
      "but        2,021\n",
      "royal      2,548\n",
      "obligations  14,422\n",
      "still      2,145\n",
      "order      2,344\n",
      "that       2,008\n",
      "a          1,037\n",
      "large      2,312\n",
      "number     2,193\n",
      "of         1,997\n",
      "the        1,996\n",
      "1          1,015\n",
      ",          1,010\n",
      "900        7,706\n",
      "or         2,030\n",
      "so         2,061\n",
      "seats      4,272\n",
      "go         2,175\n",
      "to         2,000\n",
      "guests     6,368\n",
      "from       2,013\n",
      "the        1,996\n",
      "world      2,088\n",
      "of         1,997\n",
      "politics   4,331\n",
      ",          1,010\n",
      "not        2,025\n",
      "actual     5,025\n",
      "friends    2,814\n",
      "of         1,997\n",
      "the        1,996\n",
      "couple     3,232\n",
      ".          1,012\n",
      "the        1,996\n",
      "couple     3,232\n",
      "have       2,031\n",
      "also       2,036\n",
      "invited    4,778\n",
      "many       2,116\n",
      "guests     6,368\n",
      "from       2,013\n",
      "the        1,996\n",
      "charities  15,430\n",
      "they       2,027\n",
      "work       2,147\n",
      "with       2,007\n",
      ",          1,010\n",
      "and        1,998\n",
      "middleton  17,756\n",
      "has        2,038\n",
      "used       2,109\n",
      "her        2,014\n",
      "influence   3,747\n",
      "to         2,000\n",
      "invite    13,260\n",
      "the        1,996\n",
      "butcher   14,998\n",
      ",          1,010\n",
      "shop       4,497\n",
      "##keeper  13,106\n",
      "and        1,998\n",
      "pub        9,047\n",
      "owner      3,954\n",
      "from       2,013\n",
      "her        2,014\n",
      "home       2,188\n",
      "village    2,352\n",
      "of         1,997\n",
      "buckle    22,853\n",
      "##bury     4,917\n",
      ".          1,012\n",
      "president   2,343\n",
      "barack    13,857\n",
      "obama      8,112\n",
      "and        1,998\n",
      "his        2,010\n",
      "wife       2,564\n",
      "michelle   9,393\n",
      "were       2,020\n",
      "not        2,025\n",
      "invited    4,778\n",
      "and        1,998\n",
      "many       2,116\n",
      "other      2,060\n",
      "international   2,248\n",
      "leaders    4,177\n",
      "are        2,024\n",
      "also       2,036\n",
      "expected   3,517\n",
      "to         2,000\n",
      "be         2,022\n",
      "watching   3,666\n",
      "on         2,006\n",
      "tv         2,694\n",
      ",          1,010\n",
      "not        2,025\n",
      "from       2,013\n",
      "a          1,037\n",
      "seat       2,835\n",
      "at         2,012\n",
      "westminster   9,434\n",
      "abbey      6,103\n",
      ".          1,012\n",
      "it         2,009\n",
      "is         2,003\n",
      "not        2,025\n",
      "clear      3,154\n",
      "if         2,065\n",
      "treasure   8,813\n",
      "##d        2,094\n",
      "brit      28,101\n",
      "##s        2,015\n",
      "from       2,013\n",
      "the        1,996\n",
      "world      2,088\n",
      "of         1,997\n",
      "stage      2,754\n",
      "and        1,998\n",
      "screen     3,898\n",
      "and        1,998\n",
      "pop        3,769\n",
      "music      2,189\n",
      "will       2,097\n",
      "be         2,022\n",
      "on         2,006\n",
      "the        1,996\n",
      "list       2,862\n",
      ".          1,012\n",
      "[SEP]        102\n"
     ]
    }
   ],
   "source": [
    "tokens=tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "for token, id in zip(tokens,input_ids):\n",
    "    print('{:8}{:8,}'.format(token,id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f1082b-44f4-42c2-8642-3359f34931cb",
   "metadata": {},
   "source": [
    "aynı tokenizer ve aynı model kullanıldığı sürece, belirli bir kelimenin veya token'ın ID'si hep aynı olur. Token ID'leri, modelin eğitiminde kullanılan sözlüğe (vocabulary) bağlıdır ve bu sözlük, modelle birlikte sabit olarak gelir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c334bf-1792-46c4-9574-07db2456eca6",
   "metadata": {},
   "source": [
    "Token'lar kelimelere, kelime parçalarına veya işaretlere karşılık gelir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217ef5dd-8bcb-4e35-9c80-de4da5e66036",
   "metadata": {},
   "source": [
    " we can see two special tokens [CLS] and [SEP]. [CLS] token stands for classification and is there to represent sentence-level classification and is used when we are classifying. Another token used by BERT is [SEP]. It is used to separate the two pieces of text. You can see two [SEP] tokens in the above screenshots, one after the question and another after the text.\n",
    "\n",
    " [CLS]:\n",
    "Tüm giriş dizisinin tek bir vektörde temsil edilmesini sağlar.\n",
    "Sınıflandırma, regresyon veya diğer metin düzeyindeki görevler için bu vektör kullanılabilir.\n",
    "[SEP]:\n",
    "Modelin girişteki metin bölümlerini anlamasına yardımcı olur.\n",
    "Çift metin görevlerinde, modelin hangi kısmın hangi metin olduğunu bilmesini sağlar.\n",
    "Ayrıca, cümle çiftleri arasında ilişki olup olmadığını anlamak için kullanılır (örneğin, Next Sentence Prediction görevlerinde)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff780e8c-2125-42c1-8e93-54c9493dca63",
   "metadata": {},
   "source": [
    "BERT, her token için aşağıdaki üç embedding türünü toplar:\n",
    "\n",
    "Token Embedding: Kelimenin kendisini temsil eder.\n",
    "Segment Embedding: Hangi metne ait olduğunu belirtir (soru mu, cevap mı?).\n",
    "Position Embedding: Token'ın sırasını belirtir.\n",
    "Bu üç embedding birleşerek, her token'ın bağlamda tam olarak ne anlama geldiğini modelin anlamasını sağlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c98a6d6-a0c4-45ec-b726-de6e5a17631b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEP token index: 7\n",
      "Number of tokens in segmnet A: 8\n",
      "Number of tokens in segmnet B: 387\n"
     ]
    }
   ],
   "source": [
    "#first occurance of [SEP] token\n",
    "sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
    "print(\"SEP token index:\", sep_idx)\n",
    "\n",
    "#number of tokens in segment A(question)\n",
    "num_seg_a=sep_idx+1\n",
    "print(\"Number of tokens in segmnet A:\", num_seg_a)\n",
    "\n",
    "#number of tokens in segment b (answer)\n",
    "num_seg_b= len(input_ids)-num_seg_a\n",
    "print(\"Number of tokens in segmnet B:\",num_seg_b)\n",
    "\n",
    "#creating the segment ids\n",
    "segment_ids=[0]*num_seg_a+[1]*num_seg_b\n",
    "\n",
    "#making sure that every input token has a segment id\n",
    "assert len(segment_ids)==len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d4522d2-e149-4105-857c-c7d9e75de5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model (torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0334a56b-d157-4b6c-b8b8-6c5ff26a6928",
   "metadata": {},
   "source": [
    "PyTorch tensörleri, PyTorch framework'ünde veri işlemleri ve hesaplamalar için kullanılan temel veri yapılarıdır. Tensorler, Numpy dizilerine benzer şekilde çalışır, ancak GPU hızlandırmasıyla derin öğrenme modelleri için optimize edilmiştir. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0cffc6-f9c0-4636-a6e1-3bb55232d55f",
   "metadata": {},
   "source": [
    "Girdiyi bir 2-boyutlu tensöre dönüştürmek için dış liste ([ ]) ekler.\n",
    "Modelin giriş boyutlarına uygun veri hazırlamak için kullanılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af707894-9e43-4ffd-a0ad-10e33c45fe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Question:\n",
      "Many guests are from what?\n",
      "\n",
      "Answer:\n",
      "Charities they work with\n"
     ]
    }
   ],
   "source": [
    "#tokens with highest start and end scores\n",
    "answer_start= torch.argmax(output.start_logits)\n",
    "answer_end=torch.argmax(output.end_logits)\n",
    "\n",
    "if answer_end >= answer_start:\n",
    "    answer= \" \".join(tokens[answer_start:answer_end+1])\n",
    "else:\n",
    "    print(\"I am unable to find the answer to this question. Can you please ask another question?\")\n",
    "\n",
    "print(\"\\n Question:\\n{}\".format(question.capitalize()))\n",
    "print(\"\\nAnswer:\\n{}\".format(answer.capitalize()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd5359-7dde-4c54-9b71-1ab040f51361",
   "metadata": {},
   "source": [
    "start_logits, modelin cevap olabilecek her token için tahmin ettiği başlangıç konumlarına dair puanları içerir.\n",
    "end_logits, modelin cevap olabilecek her token için tahmin ettiği bitiş konumlarına dair puanları içerir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df12d4a7-cce7-478d-9d85-06b1dc4b85bf",
   "metadata": {},
   "source": [
    "torch.argmax fonksiyonu, PyTorch'ta bir tensör üzerindeki maksimum değerin bulunduğu indeksi döndürür."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86f18f3-8a68-44fd-b6b1-01c10cd668e7",
   "metadata": {},
   "source": [
    "Consider the words, run, running, runner. Without wordpiece tokenization, the model has to store and learn the meaning of all three words independently. However, with wordpiece tokenization, each of the three words would be split into ‘run’ and the related ‘##SUFFIX’ (if any suffix at all — for example, “run”, “##ning”, “##ner”). Now, the model will learn the context of the word “run” and the rest of the meaning would be encoded in the suffix, which would be learned from other words with similar suffixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "191e2a17-d804-43de-895a-8f6b069935a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=tokens[answer_start]\n",
    "for i in range(answer_start+1, answer_end+1):\n",
    "    if tokens[i][0:2]=='##':\n",
    "        answer+=tokens[i][2:]\n",
    "    else:\n",
    "        answer+=\" \"+tokens[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98465696-8cc9-482d-8eeb-af05bf740cc5",
   "metadata": {},
   "source": [
    "turn this question-answering process into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fddd8c9-f657-4bd2-a399-eb64f65478ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer(question, text):\n",
    "\n",
    "    #tokenize question and text as a pair\n",
    "    input_ids=tokenizer.encode(question,text)\n",
    "\n",
    "    #string version of tokenized ids\n",
    "    tokens=tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    #segment ids\n",
    "    sep_idx=input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    num_seg_a=sep_idx+1\n",
    "    num_seg_b=len(input_ids)-num_seg_a\n",
    "\n",
    "    segmen_ids=[0]*num_seg_a+[1]*num_seg_b\n",
    "\n",
    "    assert len(segmen_ids)==len(input_ids)\n",
    "\n",
    "    output=model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segmen_ids]))\n",
    "\n",
    "    answer_start=torch.argmax(output.start_logits)\n",
    "    answer_end=torch.argmax(output.end_logits)\n",
    "\n",
    "    if answer_end >= answer_start:\n",
    "        answer=tokens[answer_start]\n",
    "        for i in range(answer_start+1,answer_end+1):\n",
    "            if tokens[i][0:2]=='##':\n",
    "                answer+=tokens[i][2:]\n",
    "            else:\n",
    "                answer+=\" \"+tokens[i]\n",
    "    if answer.startswith(\"[CLS]\"):\n",
    "        answer=\"Unable to find the answer to your question.\"\n",
    "    print(\"\\nPredicted answer:\\n{}\".format(answer.capitalize()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "596658e8-b801-4148-9d00-aebf9bdbf094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted answer:\n",
      "Hard rock cafe in new york ' s times square\n",
      "Original answer:\n",
      " Hard Rock Cafe\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"New York (CNN) -- More than 80 Michael Jackson collectibles -- including the late pop star's famous rhinestone-studded glove from a 1983 performance -- were auctioned off Saturday, reaping a total $2 million. Profits from the auction at the Hard Rock Cafe in New York's Times Square crushed pre-sale expectations of only $120,000 in sales. The highly prized memorabilia, which included items spanning the many stages of Jackson's career, came from more than 30 fans, associates and family members, who contacted Julien's Auctions to sell their gifts and mementos of the singer. Jackson's flashy glove was the big-ticket item of the night, fetching $420,000 from a buyer in Hong Kong, China. Jackson wore the glove at a 1983 performance during \\\"Motown 25,\\\" an NBC special where he debuted his revolutionary moonwalk. Fellow Motown star Walter \\\"Clyde\\\" Orange of the Commodores, who also performed in the special 26 years ago, said he asked for Jackson's autograph at the time, but Jackson gave him the glove instead. \"The legacy that [Jackson] left behind is bigger than life for me,\\\" Orange said. \\\"I hope that through that glove people can see what he was trying to say in his music and what he said in his music.\\\" Orange said he plans to give a portion of the proceeds to charity. Hoffman Ma, who bought the glove on behalf of Ponte 16 Resort in Macau, paid a 25 percent buyer's premium, which was tacked onto all final sales over $50,000. Winners of items less than $50,000 paid a 20 percent premium.\"\"\"\n",
    "question = \"Where was the Auction held?\"\n",
    "question_answer(question, text)\n",
    "#original answer from the dataset\n",
    "print(\"Original answer:\\n\", data.loc[data[\"question\"] == question][\"answer\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23825f1e-cc6b-4415-913d-2f32ba59d3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your text: \n",
      " The Vatican Apostolic Library (), more commonly called the Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of the oldest libraries in the world and contains one of the most significant collections of historical texts. It has 75,000 codices from throughout history, as well as 1.1 million printed books, which include some 8,500 incunabula.   The Vatican Library is a research library for history, law, philosophy, science and theology. The Vatican Library is open to anyone who can document their qualifications and research needs. Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail.   In March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts, to be made available online.   The Vatican Secret Archives were separated from the library at the beginning of the 17th century; they contain another 150,000 items.   Scholars have traditionally divided the history of the library into five periods, Pre-Lateran, Lateran, Avignon, Pre-Vatican and Vatican.   The Pre-Lateran period, comprising the initial days of the library, dated from the earliest days of the Church. Only a handful of volumes survive from this period, though some are very significant.\n",
      "\n",
      "Please enter your question: \n",
      " When was the Vat formally opened?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease enter your question: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 4\u001b[0m     question_answer(question, text)\n\u001b[0;32m      6\u001b[0m     flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     flag_N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "text = input(\"Please enter your text: \\n\")\n",
    "question = input(\"\\nPlease enter your question: \\n\")\n",
    "while True:\n",
    "    question_answer(question, text)\n",
    "    \n",
    "    flag = True\n",
    "    flag_N = False\n",
    "    \n",
    "    while flag:\n",
    "        response = input(\"\\nDo you want to ask another question based on this text (Y/N)? \")\n",
    "        if response[0] == \"Y\":\n",
    "            question = input(\"\\nPlease enter your question: \\n\")\n",
    "            flag = False\n",
    "        elif response[0] == \"N\":\n",
    "            print(\"\\nBye!\")\n",
    "            flag = False\n",
    "            flag_N = True\n",
    "            \n",
    "    if flag_N == True:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2333330-a500-4a41-8b15-fd5ef1670c53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
